# wine-quality-prediction

Jars: (Jars are provided on GitHub)
wineQuality.jar - Training application to build the model
wineQualityPrediction.jar - Prediction application using the model generated by the previous jar

Setup Security Group: 
1. In the EC2 Dashboard, go to Security Groups under Network and Security
2. Create a Security Group named 'default' that allows all inbound traffic from your local ip and all inbound traffic on the security group

Local Instance Setup:
1. Log into you AWS account and head over to the EC2 page
2. Click on Launch Instances
3. Select the Ubuntu 20.04 LTS Free Tier as you image, leave the instance tier to free type
4. Make sure the security group attached to this instance is 'default'
5. Launch the instances and connect to them using you AWS key

Setup AWS Credentials:
1. Login into instance you set up
2. Create a aws directory:
	a. mkdir /home/ubuntu/.aws
3. Create a credentials file:
	a. sudo apt-get install vim
	b. vim /home/ubuntu/.aws/credentials
	c. Copy AWS CLI information from Account Details
4. Upload aws_key.pem file to instance:
	a. Log into the AWS Console
	b. Go to the EC2 Dashboard
	c. Click Key Pairs under Network and Security
	d. Download aws_key.pem
	e. Upload to the Ubunutu instance to /home/ubuntu/.ssh/aws_key.pem
	f. In instance, run chmod 400 /home/ubuntu/.ssh/aws_key.pem

Install Flintrock:
1. Install Python:
	a. sudo apt-get install python3-pip
2. Install Flintrock:
	a. sudo pip3 install flintrock
3. Copy the yaml file from GitHub to /home/ubuntu/.config/flintrock/config.yaml
	a. sudo apt-get install vim
	b. vim /home/ubuntu/.config/flintrock/config.yaml

Setup Spark Cluster:
1. Run the command for flintrock to setup the cluster
	a. flintrock launch spark-cluster
	b. This part may take a while, sometimes up to 20 minutes, but if it looks like it's not doing anything, re-login into the local instance and move on to the next step, it should be finished
2. Get required information to upload files to master:
	a. flintrock describe spark-cluster 
		i. <SPARK_URL>: master node should have a url, so add spark://<URL>:7077 and this will be your <SPARK_URL>
		ii. <HDFS_URL>: master node should have a url, so add hdfs://<URL>:9000 and this will be your <HDFS_URL>
3. Upload TrainingDataset.csv, ValidationDataset.csv, TestDataset.csv, wineQuality.jar and wineQualityPrediction.jar to the master node at /home/ec2-user
4. Login into the cluster:
	a. flintrock login spark-cluster (Should be logged into /home/ec2-user's master instance)
5. Upload files to HDFS:
	a. cd /home/ec2-user
	b. ./hadoop/bin/hdfs dfs -copyFromLocal TrainingDataset.csv /
	c. ./hadoop/bin/hdfs dfs -copyFromLocal ValidationDataset.csv / 
	d. ./hadoop/bin/hdfs dfs -copyFromLocal TestDataset.csv /
	
Running the training application:
1. Log into the master instance if not there already:
	a. flintrock login spark-cluster
2. Run the application:
	a. ./spark/bin/spark-submit --class com.training.wineQuality.wineQualityTraining --deploy-mode client --master <SPARK_URL> wineQuality.jar <HDFS_URL>
		i. We are passing the HDFS_URL since it will be different for everyone and need to make sure the datasets are getting passed along correctly
	b. It will ask a y/n question if you want to run the ValidationDataset.csv to give validation metrics, this is optionally, just type y or n
3. Once the application completes it will create a directory called trained.model in the hdfs, we will use this for the prediction application
4. Stop the cluster, since we are finished training the model and need to conserve resources
	a. ./spark/sbin/stop-all.sh

Run predicition application locally:
1. Run the prediction application:
	a. ./spark/bin/spark-submit --class wineQualityPrediction.wineQualityPrediction --master local wineQualityPrediction.jar <HDFS_URL> TestDataset.csv
2. The output should display the F1 Scrore of the Wine Quality Prediction Application

Below are the set of instructions if you want to run the predicition application using Docker:
Install Docker:
1. Install the Docker on master:
	a. sudo yum install docker
2. Start the Docker daemon:
	a. sudo systemctl start docker
	
Use Docker to run predicition application:
1. Pull the Docker image:
	a. sudo docker pull alexandercline/spark-ready
2. Create and start the Docker image:
	a. sudo docker create -p 8081:8080 -i alexandercline/spark-ready
	b. sudo docker start <DOCKER_CONTAINER_ID>
3. Run the prediction application:
	a. sudo docker exec -ti <DOCKER_CONTAINER_ID> /bin/bash
	b. cd /opt
	c. ./spark-3.0.1-bin-hadoop2.7/bin/spark-submit --class wineQualityPrediction.wineQualityPrediction --master local wineQualityPrediction.jar <HDFS_URL> TestDataset.csv
4. The output should display the F1 Scrore of the Wine Quality Prediction Application

Delete spark-cluster:
1. Return to your Ubuntu instance you created at the begin
2. Run the following command to delete the cluster:
	a. flintrock destroy spark-cluster
3. The cluster should be deleted now
